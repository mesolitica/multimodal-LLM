{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c05c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 12 02:42:08 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              64W / 400W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              64W / 400W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              60W / 400W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              61W / 400W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-80GB          On  | 0000000B:00:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              63W / 400W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-80GB          On  | 0000000C:00:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              61W / 400W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-80GB          On  | 0000000D:00:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              63W / 400W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-80GB          On  | 0000000E:00:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              60W / 400W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b894f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_vision import MM_LLMs, MM_LLMs_Config\n",
    "from transformers import AutoModelForCausalLM, CLIPProcessor, CLIPModel,AutoModel, AutoTokenizer, AutoProcessor,AutoConfig,CLIPConfig, LlamaConfig, WhisperConfig, WhisperModel, LlamaModel, LlamaTokenizer\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from streaming import LocalDataset\n",
    "from typing import List\n",
    "from transformers import set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9b740b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vision-alignment-qwen0.5/checkpoint-13000'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "latest = get_last_checkpoint('vision-alignment-qwen0.5')\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17997f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MM_LLMs.from_pretrained(\n",
    "    latest,flash_attention = True, dtype = torch.bfloat16, torch_dtype = torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec7dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.llm.lm_head.weight = model.llm.model.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2cd492",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08fdb43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoProcessor.from_pretrained('google/siglip-base-patch16-384')\n",
    "tokenizer = AutoTokenizer.from_pretrained(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b385ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections.abc import Mapping\n",
    "\n",
    "class DataCollator():\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features):\n",
    "\n",
    "        if not isinstance(features[0], Mapping):\n",
    "            features = [vars(f) for f in features]\n",
    "\n",
    "        batch = {}\n",
    "        bs = len(features)\n",
    "        first = features[0]\n",
    "\n",
    "        batch['audio_index'] = torch.tensor([],dtype=torch.int)\n",
    "        batch['image_index'] = torch.tensor([],dtype=torch.int)\n",
    "        \n",
    "        for index, feature in enumerate(features):\n",
    "            local_index = index % (bs) \n",
    "\n",
    "            if feature['images'] is not None:\n",
    "                batch['image_index'] = torch.cat([batch['image_index'], torch.tensor([local_index] * len(feature['images']), dtype=torch.int)])\n",
    "\n",
    "        for k, v in first.items():\n",
    "\n",
    "            if k not in (\"audios\",\"images\") and not isinstance(v, str):\n",
    "                if v is None:\n",
    "                    batch[k] = None\n",
    "                elif isinstance(v, torch.Tensor):\n",
    "                    batch[k] = torch.stack([f[k] for f in features]).squeeze(1)\n",
    "                elif isinstance(v, np.ndarray):\n",
    "                    batch[k] = torch.tensor(np.stack([f[k] for f in features])).squeeze(1)\n",
    "            elif k in (\"audios\",\"images\"):\n",
    "                if v is None:\n",
    "                    batch[k] = None\n",
    "                else:         \n",
    "                    batch[k] = torch.cat([f[k] for f in features if f[k] is not None])\n",
    "\n",
    "        batch['image_starts'] = torch.tensor([self.tokenizer.convert_tokens_to_ids('<image>')] * bs, dtype=torch.int)\n",
    "        batch['image_ends'] = torch.tensor([self.tokenizer.convert_tokens_to_ids('</image>')] * bs, dtype=torch.int)\n",
    "\n",
    "        return batch\n",
    "\n",
    "collator = DataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a24e8a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = image_processor.image_processor.size['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e2279b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(messages, images: List[str] = None):\n",
    "    if images is not None:\n",
    "        images = [Image.open(f).convert('RGB') for f in images]\n",
    "        image_output = image_processor(images=images, return_tensors='pt')['pixel_values']\n",
    "    else:\n",
    "        image_output = None\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize = False)\n",
    "    outputs = tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors='pt',\n",
    "                    return_overflowing_tokens=False,\n",
    "                    return_length=False)\n",
    "\n",
    "    outputs['images'] = image_output\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46803884",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'content': '<image> </image> ini gambar apa'},\n",
    "]\n",
    "outputs = prepare_dataset(messages, images = ['motosikal.jpeg'])\n",
    "ok = collator([outputs])\n",
    "ok['labels'] = ok['input_ids']\n",
    "\n",
    "for k in ok.keys():\n",
    "    if ok[k] is not None:\n",
    "        ok[k] = ok[k].cuda()\n",
    "        \n",
    "for k in ['images']:\n",
    "    if ok[k] is not None:\n",
    "        ok[k] = ok[k].type(model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1830b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12]), torch.Size([1, 588, 1024]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_inputs = model.prepare_inputs_for_generation(**ok)\n",
    "r = model_inputs.pop('input_ids', None)\n",
    "label = model_inputs.pop('labels', None)\n",
    "label = label.detach().cpu().numpy()\n",
    "ok['input_ids'].shape, model_inputs['inputs_embeds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "637355b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "r = model.llm.generate(input_ids = ok['input_ids'], max_new_tokens=20,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdcdcfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\n<image> </image> ini gambar apa<|im_end|>\\n<|im_start|>assistant\\nSaya tidak dapat memberikan anda kod yang berkaitan dengan jenis khusus'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "718c0b49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/utils/hub.py:667: UserWarning: The `organization` argument is deprecated and will be removed in v5 of Transformers. Set your organization directly in the `repo_id` passed instead (`repo_id={organization}/{model_id}`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b6614310b54afdb6ca931d24945f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/malaysian-Qwen1.5-0.5B-siglip-base-384-vision-alignment/commit/1f02e3d8eea28541affa4e5a68fb133c9450f5e7', commit_message='Upload MM_LLMs', commit_description='', oid='1f02e3d8eea28541affa4e5a68fb133c9450f5e7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('malaysian-Qwen1.5-0.5B-siglip-base-384-vision-alignment', \n",
    "                  organization='mesolitica', safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98862dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/utils/hub.py:667: UserWarning: The `organization` argument is deprecated and will be removed in v5 of Transformers. Set your organization directly in the `repo_id` passed instead (`repo_id={organization}/{model_id}`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/malaysian-Qwen1.5-0.5B-siglip-base-384-vision-alignment/commit/35e8fca1972a6ace330c1237da34735e326e7284', commit_message='Upload processor', commit_description='', oid='35e8fca1972a6ace330c1237da34735e326e7284', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor.push_to_hub('malaysian-Qwen1.5-0.5B-siglip-base-384-vision-alignment', organization='mesolitica', safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa9233d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/malaysian-Qwen1.5-0.5B-siglip-base-384-vision-alignment/commit/f569f4d76186bf4d6f4cd8356d02a4bb1c71cfaa', commit_message='Upload tokenizer', commit_description='', oid='f569f4d76186bf4d6f4cd8356d02a4bb1c71cfaa', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('malaysian-Qwen1.5-0.5B-siglip-base-384-vision-alignment', organization='mesolitica', safe_serialization=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
