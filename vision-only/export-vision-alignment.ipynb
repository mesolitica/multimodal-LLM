{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3adb21bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb  9 03:32:33 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   55C    P0             342W / 400W |  63249MiB / 81920MiB |     71%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   49C    P0             348W / 400W |  36483MiB / 81920MiB |     37%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   51C    P0             301W / 400W |  63339MiB / 81920MiB |     62%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   50C    P0             346W / 400W |  70477MiB / 81920MiB |     31%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-80GB          On  | 0000000B:00:00.0 Off |                    0 |\n",
      "| N/A   54C    P0             259W / 400W |  61563MiB / 81920MiB |     48%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-80GB          On  | 0000000C:00:00.0 Off |                    0 |\n",
      "| N/A   51C    P0             252W / 400W |  61531MiB / 81920MiB |     50%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-80GB          On  | 0000000D:00:00.0 Off |                    0 |\n",
      "| N/A   54C    P0             290W / 400W |  63339MiB / 81920MiB |     96%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-80GB          On  | 0000000E:00:00.0 Off |                    0 |\n",
      "| N/A   52C    P0             347W / 400W |  63681MiB / 81920MiB |     55%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea04392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_vision import MM_LLMs, MM_LLMs_Config\n",
    "from transformers import CLIPProcessor, CLIPModel,AutoModel, AutoTokenizer, AutoProcessor,AutoConfig,CLIPConfig, LlamaConfig, WhisperConfig, WhisperModel, LlamaModel, LlamaTokenizer\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from streaming import LocalDataset\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f79c8b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vision-alignment-qwen0.5/checkpoint-400'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "latest = get_last_checkpoint('vision-alignment-qwen0.5')\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b0d503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MM_LLMs were not initialized from the model checkpoint at vision-alignment-qwen0.5/checkpoint-400 and are newly initialized: ['llm.lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = MM_LLMs.from_pretrained(\n",
    "    latest,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "090839c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoProcessor.from_pretrained('google/siglip-base-patch16-384')\n",
    "tokenizer = AutoTokenizer.from_pretrained(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d7e2fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='vision-alignment-qwen0.5/checkpoint-400', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t151647: AddedToken(\"</image>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t151648: AddedToken(\"<audio>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t151649: AddedToken(\"</audio>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96227365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"eos_token_id\": 151645\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.llm.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "model.llm.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac6f8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections.abc import Mapping\n",
    "\n",
    "class DataCollator():\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features):\n",
    "\n",
    "        if not isinstance(features[0], Mapping):\n",
    "            features = [vars(f) for f in features]\n",
    "\n",
    "        batch = {}\n",
    "        bs = len(features)\n",
    "        first = features[0]\n",
    "\n",
    "        batch['audio_index'] = torch.tensor([],dtype=torch.int)\n",
    "        batch['image_index'] = torch.tensor([],dtype=torch.int)\n",
    "        \n",
    "        for index, feature in enumerate(features):\n",
    "            local_index = index % (bs) \n",
    "\n",
    "            if feature['images'] is not None:\n",
    "                batch['image_index'] = torch.cat([batch['image_index'], torch.tensor([local_index] * len(feature['images']), dtype=torch.int)])\n",
    "\n",
    "        for k, v in first.items():\n",
    "\n",
    "            if k not in (\"audios\",\"images\") and not isinstance(v, str):\n",
    "                if v is None:\n",
    "                    batch[k] = None\n",
    "                elif isinstance(v, torch.Tensor):\n",
    "                    batch[k] = torch.stack([f[k] for f in features]).squeeze(1)\n",
    "                elif isinstance(v, np.ndarray):\n",
    "                    batch[k] = torch.tensor(np.stack([f[k] for f in features])).squeeze(1)\n",
    "            elif k in (\"audios\",\"images\"):\n",
    "                if v is None:\n",
    "                    batch[k] = None\n",
    "                else:         \n",
    "                    batch[k] = torch.cat([f[k] for f in features if f[k] is not None])\n",
    "\n",
    "        batch['image_starts'] = torch.tensor([self.tokenizer.convert_tokens_to_ids('<image>')] * bs, dtype=torch.int)\n",
    "        batch['image_ends'] = torch.tensor([self.tokenizer.convert_tokens_to_ids('</image>')] * bs, dtype=torch.int)\n",
    "\n",
    "        return batch\n",
    "\n",
    "collator = DataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c72297",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = image_processor.image_processor.size['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c329242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(messages, images: List[str] = None):\n",
    "    if images is not None:\n",
    "        images = [Image.open(f) for f in images]\n",
    "        image_output = image_processor(images=images, return_tensors='pt')['pixel_values']\n",
    "    else:\n",
    "        image_output = None\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize = False)\n",
    "    outputs = tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors='pt',\n",
    "                    return_overflowing_tokens=False,\n",
    "                    return_length=False)\n",
    "\n",
    "    outputs['images'] = image_output\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4034c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'content': '<image> </image> ini gambar apa'},\n",
    "]\n",
    "outputs = prepare_dataset(messages, images = ['motosikal.jpeg'])\n",
    "ok = collator([outputs])\n",
    "ok['labels'] = ok['input_ids']\n",
    "\n",
    "# for k in ok.keys():\n",
    "#     if ok[k] is not None:\n",
    "#         ok[k] = ok[k].cuda()\n",
    "        \n",
    "# for k in ['audios', 'images']:\n",
    "#     if ok[k] is not None:\n",
    "#         ok[k] = ok[k].type(model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a12aa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12]), torch.Size([1, 588, 1024]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_inputs = model.prepare_inputs_for_generation(**ok)\n",
    "r = model_inputs.pop('input_ids', None)\n",
    "label = model_inputs.pop('labels', None)\n",
    "label = label.detach().cpu().numpy()\n",
    "ok['input_ids'].shape, model_inputs['inputs_embeds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c26bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.type(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d37dc658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/utils/hub.py:667: UserWarning: The `organization` argument is deprecated and will be removed in v5 of Transformers. Set your organization directly in the `repo_id` passed instead (`repo_id={organization}/{model_id}`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3e21dd003f4213a0cdc3eb3dcc5685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.65G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/malaysian-Qwen1.5-0.5B-vision-alignment/commit/95621446b08c1b4e877db9df08dcc536d566a5b7', commit_message='Upload MM_LLMs', commit_description='', oid='95621446b08c1b4e877db9df08dcc536d566a5b7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('malaysian-Qwen1.5-0.5B-vision-alignment', organization='mesolitica', safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f48160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/utils/hub.py:667: UserWarning: The `organization` argument is deprecated and will be removed in v5 of Transformers. Set your organization directly in the `repo_id` passed instead (`repo_id={organization}/{model_id}`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/malaysian-Qwen1.5-0.5B-vision-alignment/commit/9e75b4e0b1774489eb1a61658bb1ae1ed817a13b', commit_message='Upload processor', commit_description='', oid='9e75b4e0b1774489eb1a61658bb1ae1ed817a13b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor.push_to_hub('malaysian-Qwen1.5-0.5B-vision-alignment', organization='mesolitica', safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8452a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/malaysian-Qwen1.5-0.5B-vision-alignment/commit/32005ef017a0627a52b5545f8f962bbf20a31bdc', commit_message='Upload tokenizer', commit_description='', oid='32005ef017a0627a52b5545f8f962bbf20a31bdc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('malaysian-Qwen1.5-0.5B-vision-alignment', organization='mesolitica', safe_serialization=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
