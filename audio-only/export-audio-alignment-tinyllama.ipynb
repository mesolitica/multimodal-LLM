{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c05c697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 13 07:28:12 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   62C    P0             407W / 400W |  63225MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   57C    P0             416W / 400W |  74505MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   59C    P0             356W / 400W |  68685MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   48C    P0              96W / 400W |  51873MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-80GB          On  | 0000000B:00:00.0 Off |                    0 |\n",
      "| N/A   51C    P0             103W / 400W |  76817MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-80GB          On  | 0000000C:00:00.0 Off |                    0 |\n",
      "| N/A   58C    P0             438W / 400W |  66959MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-80GB          On  | 0000000D:00:00.0 Off |                    0 |\n",
      "| N/A   58C    P0             122W / 400W |  56653MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-80GB          On  | 0000000E:00:00.0 Off |                    0 |\n",
      "| N/A   48C    P0              92W / 400W |  72115MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c66e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b894f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_audio import MM_LLMs, MM_LLMs_Config\n",
    "from transformers import AutoModelForCausalLM, CLIPProcessor, CLIPModel,AutoModel, AutoTokenizer, AutoProcessor,AutoConfig,CLIPConfig, LlamaConfig, WhisperConfig, WhisperModel, LlamaModel, LlamaTokenizer\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from streaming import LocalDataset\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe74f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "MM_LLMs.register_for_auto_class()\n",
    "MM_LLMs_Config.register_for_auto_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f9b740b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audio-alignment-tinyllama/checkpoint-5200'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "latest = get_last_checkpoint('audio-alignment-tinyllama')\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b6dd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n"
     ]
    }
   ],
   "source": [
    "model = MM_LLMs.from_pretrained(\n",
    "    latest,flash_attention = True, dtype = torch.bfloat16, torch_dtype = torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56e59fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fdb43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "audio_processor = AutoProcessor.from_pretrained('mesolitica/malaysian-whisper-small')\n",
    "tokenizer = AutoTokenizer.from_pretrained(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cced9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.llm.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "model.llm.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b385ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections.abc import Mapping\n",
    "\n",
    "class DataCollator():\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features):\n",
    "\n",
    "        if not isinstance(features[0], Mapping):\n",
    "            features = [vars(f) for f in features]\n",
    "\n",
    "        batch = {}\n",
    "        bs = len(features)\n",
    "        first = features[0]\n",
    "\n",
    "        batch['audio_index'] = torch.tensor([],dtype=torch.int)\n",
    "        \n",
    "        for index, feature in enumerate(features):\n",
    "            local_index = index % (bs) \n",
    "\n",
    "            if feature['audios'] is not None:\n",
    "                batch['audio_index'] = torch.cat([batch['audio_index'], torch.tensor(\n",
    "                    [local_index] * len(feature['audios']), dtype=torch.int)])\n",
    "\n",
    "        for k, v in first.items():\n",
    "\n",
    "            if k not in (\"audios\",\"images\") and not isinstance(v, str):\n",
    "                if v is None:\n",
    "                    batch[k] = None\n",
    "                elif isinstance(v, torch.Tensor):\n",
    "                    batch[k] = torch.stack([f[k] for f in features]).squeeze(1)\n",
    "                elif isinstance(v, np.ndarray):\n",
    "                    batch[k] = torch.tensor(np.stack([f[k] for f in features])).squeeze(1)\n",
    "            elif k in (\"audios\",\"images\"):\n",
    "                if v is None:\n",
    "                    batch[k] = None\n",
    "                else:         \n",
    "                    batch[k] = torch.cat([f[k] for f in features if f[k] is not None])\n",
    "\n",
    "        batch['audio_starts'] = torch.tensor(\n",
    "            [self.tokenizer.convert_tokens_to_ids('<audio>')] * bs, dtype=torch.int)\n",
    "        batch['audio_ends'] = torch.tensor(\n",
    "            [self.tokenizer.convert_tokens_to_ids('</audio>')] * bs, dtype=torch.int)\n",
    "\n",
    "        return batch\n",
    "\n",
    "collator = DataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e2279b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(messages, audio: List[str] = None, sr = 16000):\n",
    "    if audio is not None:\n",
    "        audio = [librosa.load(f, sr=sr)[0] for f in audio]\n",
    "        audio_features = audio_processor(audio, sampling_rate=sr, return_tensors='pt',)['input_features']\n",
    "    else:\n",
    "        audio_features = None\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize = False)\n",
    "    outputs = tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors='pt',\n",
    "                    return_overflowing_tokens=False,\n",
    "                    return_length=False)\n",
    "\n",
    "    outputs['audios'] = audio_features\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46803884",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'content': '<audio> </audio> audio ni tentang apa'},\n",
    "]\n",
    "outputs = prepare_dataset(messages, audio = ['test.mp3'])\n",
    "ok = collator([outputs])\n",
    "ok['labels'] = ok['input_ids']\n",
    "\n",
    "for k in ok.keys():\n",
    "    if ok[k] is not None:\n",
    "        ok[k] = ok[k].cuda()\n",
    "        \n",
    "for k in ['audios']:\n",
    "    if ok[k] is not None:\n",
    "        ok[k] = ok[k].type(model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1830b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 16]), torch.Size([1, 503, 2048]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_inputs = model.prepare_inputs_for_generation(**ok)\n",
    "r = model_inputs.pop('input_ids', None)\n",
    "label = model_inputs.pop('labels', None)\n",
    "label = label.detach().cpu().numpy()\n",
    "ok['input_ids'].shape, model_inputs['inputs_embeds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "718c0b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8247a55d7744b38d65a486fbf9a839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/malaysian-tinyllama-1.1b-audio-alignment/commit/b7ea246e3865a8fc18978ceef560729de603f722', commit_message='Upload MM_LLMs', commit_description='', oid='b7ea246e3865a8fc18978ceef560729de603f722', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('malaysian-tinyllama-1.1b-audio-alignment', organization='mesolitica', safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98862dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/utils/hub.py:667: UserWarning: The `organization` argument is deprecated and will be removed in v5 of Transformers. Set your organization directly in the `repo_id` passed instead (`repo_id={organization}/{model_id}`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7e76980088460598e01bedd6c70985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/malaysian-tinyllama-1.1b-audio-alignment/commit/331fa4edc4d18640f77867582d4ded6e25b86054', commit_message='Upload processor', commit_description='', oid='331fa4edc4d18640f77867582d4ded6e25b86054', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_processor.push_to_hub('malaysian-tinyllama-1.1b-audio-alignment', organization='mesolitica', safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfa9233d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/malaysian-tinyllama-1.1b-audio-alignment/commit/18c43ac085404fe2d86436a0ae71d2e85a9cc92c', commit_message='Upload tokenizer', commit_description='', oid='18c43ac085404fe2d86436a0ae71d2e85a9cc92c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('malaysian-tinyllama-1.1b-audio-alignment', organization='mesolitica', safe_serialization=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
